{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":73206,"databundleVersionId":8057312,"sourceType":"competition"},{"sourceId":8304707,"sourceType":"datasetVersion","datasetId":4933343},{"sourceId":169622515,"sourceType":"kernelVersion"}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/velimirkovacic/mn-0036533917-ferules?scriptVersionId=183359087\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Feature engineering, rules-based models and ensembles\n","metadata":{}},{"cell_type":"code","source":"!pip install Wittgenstein","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:25:52.744244Z","iopub.execute_input":"2024-05-05T19:25:52.744647Z","iopub.status.idle":"2024-05-05T19:26:04.438529Z","shell.execute_reply.started":"2024-05-05T19:25:52.744615Z","shell.execute_reply":"2024-05-05T19:26:04.437226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import f1_score\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import LinearSVC\nimport shap\nimport wittgenstein as lw\nfrom IPython.display import display\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:26:04.440988Z","iopub.execute_input":"2024-05-05T19:26:04.44134Z","iopub.status.idle":"2024-05-05T19:26:04.448631Z","shell.execute_reply.started":"2024-05-05T19:26:04.441298Z","shell.execute_reply":"2024-05-05T19:26:04.447511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First we will import the train dataset created in the first notebook. ","metadata":{}},{"cell_type":"code","source":"\ndf_train = pd.read_csv(\"../input/train-set-za-biljeznicu-2/train2.csv\")\ndf_train['Date'] = pd.to_datetime(df_train['Date'])\n\ndf_test = pd.read_csv(\"../input/dapprojekt24-1/test.csv\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:26:04.450011Z","iopub.execute_input":"2024-05-05T19:26:04.450405Z","iopub.status.idle":"2024-05-05T19:26:05.743601Z","shell.execute_reply.started":"2024-05-05T19:26:04.450371Z","shell.execute_reply":"2024-05-05T19:26:05.742637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Cross-validation Function","metadata":{}},{"cell_type":"markdown","source":"The cross-validation function uses a five-fold cross-validation. The function takes a dataset (X y), splits it into 5 folds, tests 5 classifiers with default parameters and prints a table showing f1-scores. The calssifiers are: GaussianNB, LogisticRegression, RandomForestClassifier, ExtraTreesClassifier and XGBClassifier. ","metadata":{}},{"cell_type":"code","source":"def test_model(model, X_train, X_test, y_train, y_test):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    return f1_score(y_test, y_pred)\n\n\ndef cross_val_5fold(X, y):\n    models = [\n    (GaussianNB(), \"GaussianNB\"),\n    (LogisticRegression(), \"LogisticRegression\"),\n    (RandomForestClassifier(), \"RandomForestClassifier\"),\n    (ExtraTreesClassifier(), \"ExtraTreesClassifier\"),\n    (XGBClassifier(), \"XGBClassifier\")\n    ]\n\n    data = {\n            'Algorithm': [],\n            'Fold 1': [],\n            'Fold 2': [],\n            'Fold 3': [],\n            'Fold 4': [],\n            'Fold 5': [],\n            'Average': []\n        }\n    df = pd.DataFrame(data)\n\n\n    for model, name in models: \n        print(\"Model:\", name)\n        five_folds = KFold(n_splits=5, shuffle=True)\n        row = [name]\n        i = 1\n        for train, test in five_folds.split(X):\n            X_train = X.iloc[train]\n            X_test = X.iloc[test]\n            y_train = y.iloc[train]\n            y_test = y.iloc[test] \n            print(\"Fold\", i)\n            i+=1\n            f1 = test_model(model, X_train, X_test, y_train, y_test)\n            row += [f1]\n        \n        row += [np.average(row[1:])]\n        df.loc[len(df)] = row\n    print(df)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:26:05.745104Z","iopub.execute_input":"2024-05-05T19:26:05.745585Z","iopub.status.idle":"2024-05-05T19:26:05.758312Z","shell.execute_reply.started":"2024-05-05T19:26:05.745541Z","shell.execute_reply":"2024-05-05T19:26:05.757314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Running the cross-validation function\nWe will use the five-fold cross-validation function on the default dataset.","metadata":{}},{"cell_type":"code","source":"X = df_train.drop(columns=[\"Date\", \"Symbol\", \"Id\", \"Target\"])\ny = df_train[\"Target\"]\ncross_val_5fold(X, y)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:26:05.761547Z","iopub.execute_input":"2024-05-05T19:26:05.761895Z","iopub.status.idle":"2024-05-05T19:26:05.794197Z","shell.execute_reply.started":"2024-05-05T19:26:05.761866Z","shell.execute_reply":"2024-05-05T19:26:05.793116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Submitting to Competition\nThe Random Forest Classifier has achieved the highest F1 score and we will thus submit it to competition as per the instructions. NaN values in the test set have been resolved using mean-fill.","metadata":{}},{"cell_type":"code","source":"X_test = df_test.drop(columns=[\"Date\", \"Symbol\", \"Id\"])\nX_test = X_test.fillna(X_test.mean())\nmodel = RandomForestClassifier()\nmodel.fit(X, y)\ny_test = model.predict(X_test)\nsubmission = df_test.loc[:,df_test.columns.isin(('Id', ))]\nsubmission.loc[:,'Target'] = y_test\nsubmission.to_csv(\"submission_1.csv\", index=None)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:26:05.795554Z","iopub.execute_input":"2024-05-05T19:26:05.795909Z","iopub.status.idle":"2024-05-05T19:26:05.800704Z","shell.execute_reply.started":"2024-05-05T19:26:05.79588Z","shell.execute_reply":"2024-05-05T19:26:05.799701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. New Features\n### 2.1 Domain specific features\n#### 2.1.1 Simple Moving Average SMA\nSMA is calculated as the average closing price of the stock last N days. We will use N = 200 as it is the most commomn within trading circles. The 199 NaNs per stock will be filled with the Close values.","metadata":{}},{"cell_type":"code","source":"df_train['SMA'] = df_train.groupby('Symbol')['Close'].rolling(window=200).mean().reset_index(0, drop=True)\ndf_train.loc[df_train['SMA'].isna(), 'SMA'] = df_train['Close']\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:26:05.801962Z","iopub.execute_input":"2024-05-05T19:26:05.802296Z","iopub.status.idle":"2024-05-05T19:26:05.976786Z","shell.execute_reply.started":"2024-05-05T19:26:05.802264Z","shell.execute_reply":"2024-05-05T19:26:05.975765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.1.2 Exponential Moving Average (EMA)\nUnlike SMA, EMA places greater importance on more recent values. It is a weighted average. We will use a 12 day span.","metadata":{}},{"cell_type":"code","source":"df_train['EMA'] = df_train.groupby('Symbol')['Close'].apply(lambda x: x.ewm(span=12, adjust=False).mean()).reset_index(0, drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:26:05.978368Z","iopub.execute_input":"2024-05-05T19:26:05.978678Z","iopub.status.idle":"2024-05-05T19:26:06.167668Z","shell.execute_reply.started":"2024-05-05T19:26:05.978652Z","shell.execute_reply":"2024-05-05T19:26:06.166641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.1.3 Moving Average Convergence Divergence (MACD)\nMACD is the difference between a 12 day span EMA and a 26 day span EMA. ","metadata":{}},{"cell_type":"code","source":"df_train['MACD'] = df_train['EMA'] - df_train.groupby('Symbol')['Close'].apply(lambda x: x.ewm(span=26, adjust=False).mean()).reset_index(0, drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:26:06.16916Z","iopub.execute_input":"2024-05-05T19:26:06.169482Z","iopub.status.idle":"2024-05-05T19:26:06.384578Z","shell.execute_reply.started":"2024-05-05T19:26:06.169453Z","shell.execute_reply":"2024-05-05T19:26:06.383615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.1.4 Relative Strength Index (RSI)\nRSI is a an indicator within range of 0 to 100 and is calculated as: \n$RSI = 100 - \\frac{100}{1 + \\frac{AvgGain}{AvgLoss}}.$ We will use a 14 day span.\n\nThe missing values will be filled with the mean value of that stock. We will do this for all other instances of this issue.\n\n","metadata":{}},{"cell_type":"code","source":"change = df_train.groupby('Symbol')['Close'].diff()\n\ndf_train['Gain'] = change.apply(lambda x: x if x > 0 else 0)\ndf_train['Loss'] = change.apply(lambda x: -x if x < 0 else 0)\n\navgGain = df_train.groupby('Symbol')['Gain'].rolling(window=14, min_periods=1).mean().reset_index(drop=True)\navgLoss = df_train.groupby('Symbol')['Loss'].rolling(window=14, min_periods=1).mean().reset_index(drop=True)\n\n\ndf_train['RSI'] = 100 - (100 / (1 + avgGain/avgLoss))\ndf_train.loc[df_train['Loss'] < 1e-2, 'RSI'] = 100\ndf_train[\"RSI\"] = df_train.groupby('Symbol')['RSI'].transform(lambda x: x.fillna(x.mean()))\ndf_train = df_train.drop(columns=['Gain', 'Loss'])","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:26:06.385666Z","iopub.execute_input":"2024-05-05T19:26:06.385981Z","iopub.status.idle":"2024-05-05T19:26:07.205308Z","shell.execute_reply.started":"2024-05-05T19:26:06.385954Z","shell.execute_reply":"2024-05-05T19:26:07.204339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.1.5 On-Balance Volume (OBV)\nOBV is calculated as:\n\n$\nOBV_i = OBV_{i -1} \\begin{cases} \nVolume_i & \\text{if } Close_i > Close_{i-1} \\\\\n0 & \\text{if } Close_i = Close_{i-1} \\\\\n-Volume_i & \\text{if } Close_i < Close_{i-1} \n\\end{cases}\n$","metadata":{}},{"cell_type":"code","source":"df_train['Dir'] = change.apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\ndf_train['OBV'] = df_train.groupby('Symbol')['Volume'].transform(lambda x: x * df_train['Dir']).cumsum()\n\ndf_train = df_train.drop(columns=['Dir'])","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:26:07.206899Z","iopub.execute_input":"2024-05-05T19:26:07.207266Z","iopub.status.idle":"2024-05-05T19:26:11.284164Z","shell.execute_reply.started":"2024-05-05T19:26:07.207233Z","shell.execute_reply":"2024-05-05T19:26:11.283297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:26:11.285211Z","iopub.execute_input":"2024-05-05T19:26:11.285491Z","iopub.status.idle":"2024-05-05T19:26:11.315115Z","shell.execute_reply.started":"2024-05-05T19:26:11.285468Z","shell.execute_reply":"2024-05-05T19:26:11.314103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 History related features\n","metadata":{}},{"cell_type":"markdown","source":"Here we will invent new history related features that may help in our predictions.","metadata":{}},{"cell_type":"markdown","source":"### 2.2.1 Close Entropy (CE)\nWe will calculate entropy of closing prices. We will use a 10 day span. Missing entropies will be filled by the mean of the specific stock.","metadata":{}},{"cell_type":"code","source":"probs = df_train.groupby('Symbol')['Close'].transform(lambda x: x / x.sum())\n\ndf_train[\"CE\"] = probs.rolling(window=10).apply(lambda x: -np.sum(x * np.log2(x))).reset_index(drop=True)\ndf_train[\"CE\"] = df_train.groupby('Symbol')['CE'].transform(lambda x: x.fillna(x.mean()))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:26:11.31654Z","iopub.execute_input":"2024-05-05T19:26:11.316852Z","iopub.status.idle":"2024-05-05T19:28:13.626408Z","shell.execute_reply.started":"2024-05-05T19:26:11.316827Z","shell.execute_reply":"2024-05-05T19:28:13.625463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.2 Average Open-Close (AOC)\nThis will be the average value of Open - Close. We will use a 20 day span.","metadata":{}},{"cell_type":"code","source":"open_mean = df_train.groupby('Symbol')['Open'].rolling(window=20).mean().reset_index(drop=True)\nclose_mean = df_train.groupby('Symbol')['Close'].rolling(window=20).mean().reset_index(drop=True)\n\ndf_train[\"AOC\"] = open_mean - close_mean\ndf_train[\"AOC\"] = df_train.groupby('Symbol')['AOC'].transform(lambda x: x.fillna(x.mean()))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:28:13.630855Z","iopub.execute_input":"2024-05-05T19:28:13.631175Z","iopub.status.idle":"2024-05-05T19:28:14.023251Z","shell.execute_reply.started":"2024-05-05T19:28:13.631147Z","shell.execute_reply":"2024-05-05T19:28:14.022376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.3 Close Variance (CV)\nThis will be the variance of Close. We will use a 30 day span.\n","metadata":{}},{"cell_type":"code","source":"df_train[\"CV\"] = df_train.groupby('Symbol')[\"Close\"].rolling(window=30).var().reset_index(drop=True)\ndf_train[\"CV\"] = df_train.groupby('Symbol')[\"CV\"].transform(lambda x: x.fillna(x.mean()))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:28:14.024611Z","iopub.execute_input":"2024-05-05T19:28:14.025093Z","iopub.status.idle":"2024-05-05T19:28:14.278472Z","shell.execute_reply.started":"2024-05-05T19:28:14.025056Z","shell.execute_reply":"2024-05-05T19:28:14.277621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.4 Mean Volume (MV)\nThis will be the mean of Volume. We will use a 15 day span.\n","metadata":{}},{"cell_type":"code","source":"df_train[\"MV\"] = df_train.groupby('Symbol')[\"Volume\"].rolling(window=15).mean().reset_index(drop=True)\ndf_train[\"MV\"] = df_train.groupby('Symbol')[\"MV\"].transform(lambda x: x.fillna(x.mean()))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:28:14.279687Z","iopub.execute_input":"2024-05-05T19:28:14.280008Z","iopub.status.idle":"2024-05-05T19:28:14.52686Z","shell.execute_reply.started":"2024-05-05T19:28:14.279982Z","shell.execute_reply":"2024-05-05T19:28:14.526004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.5 Volume Variance (VV)\nThis is the variance of Volume. We will use a 30 day span.","metadata":{}},{"cell_type":"code","source":"df_train[\"VV\"] = df_train.groupby('Symbol')[\"Volume\"].rolling(window=30).var().reset_index(drop=True)\ndf_train[\"VV\"] = df_train.groupby('Symbol')[\"VV\"].transform(lambda x: x.fillna(x.mean()))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:28:14.528159Z","iopub.execute_input":"2024-05-05T19:28:14.528434Z","iopub.status.idle":"2024-05-05T19:28:14.781541Z","shell.execute_reply.started":"2024-05-05T19:28:14.52841Z","shell.execute_reply":"2024-05-05T19:28:14.780491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.6 Average High-Low (AHL)\nThis will be the average value of High - Low. We will use a 20 day span.","metadata":{}},{"cell_type":"code","source":"high_mean = df_train.groupby('Symbol')['High'].rolling(window=20).mean().reset_index(drop=True)\nlow_mean = df_train.groupby('Symbol')['Low'].rolling(window=20).mean().reset_index(drop=True)\n\ndf_train[\"AHL\"] = high_mean - low_mean\ndf_train[\"AHL\"] = df_train.groupby('Symbol')['AHL'].transform(lambda x: x.fillna(x.mean()))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:28:14.782928Z","iopub.execute_input":"2024-05-05T19:28:14.783257Z","iopub.status.idle":"2024-05-05T19:28:15.15705Z","shell.execute_reply.started":"2024-05-05T19:28:14.78323Z","shell.execute_reply":"2024-05-05T19:28:15.156231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.7 High Variance (HV)\nThis is the variance of High. We will use a 20 day span.","metadata":{}},{"cell_type":"code","source":"df_train[\"HV\"] = df_train.groupby('Symbol')['High'].rolling(window=20).var().reset_index(drop=True)\ndf_train[\"HV\"] = df_train.groupby('Symbol')['HV'].transform(lambda x: x.fillna(x.mean()))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:28:15.158266Z","iopub.execute_input":"2024-05-05T19:28:15.158654Z","iopub.status.idle":"2024-05-05T19:28:15.409857Z","shell.execute_reply.started":"2024-05-05T19:28:15.158622Z","shell.execute_reply":"2024-05-05T19:28:15.40847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.8 Low Variance (LV)\nThis is the variance of Low. We will use a 20 day span.","metadata":{}},{"cell_type":"code","source":"df_train[\"LV\"] = df_train.groupby('Symbol')['Low'].rolling(window=20).var().reset_index(drop=True)\ndf_train[\"LV\"] = df_train.groupby('Symbol')['LV'].transform(lambda x: x.fillna(x.mean()))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:28:15.411386Z","iopub.execute_input":"2024-05-05T19:28:15.411935Z","iopub.status.idle":"2024-05-05T19:28:15.663683Z","shell.execute_reply.started":"2024-05-05T19:28:15.411892Z","shell.execute_reply":"2024-05-05T19:28:15.662856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.9 Volume Entropy (VE)\nThis is the entropy of Volume. We will use a 10 day span.","metadata":{}},{"cell_type":"code","source":"probs = df_train.groupby('Symbol')['Volume'].transform(lambda x: x / x.sum())\n\ndf_train[\"VE\"] = probs.rolling(window=10).apply(lambda x: -np.sum(x/np.sum(x) * np.log2(x/np.sum(x)))).reset_index(drop=True)\ndf_train[\"VE\"] = df_train.groupby('Symbol')['VE'].transform(lambda x: x.fillna(x.mean()))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:28:15.664932Z","iopub.execute_input":"2024-05-05T19:28:15.665243Z","iopub.status.idle":"2024-05-05T19:32:24.26317Z","shell.execute_reply.started":"2024-05-05T19:28:15.665218Z","shell.execute_reply":"2024-05-05T19:32:24.262072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2.10 Open Entropy (OE)\nThis is the entropy of Open. We will use a 10 day span.","metadata":{}},{"cell_type":"code","source":"probs = df_train.groupby('Symbol')['Open'].transform(lambda x: x / x.sum())\n\ndf_train[\"OE\"] = probs.rolling(window=10).apply(lambda x: -np.sum(x/np.sum(x) * np.log2(x/np.sum(x)))).reset_index(drop=True)\ndf_train[\"OE\"] = df_train.groupby('Symbol')['OE'].transform(lambda x: x.fillna(x.mean()))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:32:24.264452Z","iopub.execute_input":"2024-05-05T19:32:24.264755Z","iopub.status.idle":"2024-05-05T19:36:30.006498Z","shell.execute_reply.started":"2024-05-05T19:32:24.26473Z","shell.execute_reply":"2024-05-05T19:36:30.00535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:36:30.007882Z","iopub.execute_input":"2024-05-05T19:36:30.00821Z","iopub.status.idle":"2024-05-05T19:36:30.443666Z","shell.execute_reply.started":"2024-05-05T19:36:30.008183Z","shell.execute_reply":"2024-05-05T19:36:30.44269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalization\n\nWe will create a new dataset with the 15 new features and then normalize them with Min-Max normalization.","metadata":{}},{"cell_type":"code","source":"df = df_train.drop(columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Id\", \"Date\", \"Volume\"])\n\nfor i in df.columns:\n    if i == 'Symbol':\n        continue\n    df[i] = df.groupby('Symbol')[i].transform(lambda x: ((x - x.min() )/ (x.max() - x.min()))).reset_index(drop=True)\n\ndf = df.drop(columns=['Symbol'])","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:36:30.445691Z","iopub.execute_input":"2024-05-05T19:36:30.446521Z","iopub.status.idle":"2024-05-05T19:36:33.64035Z","shell.execute_reply.started":"2024-05-05T19:36:30.446484Z","shell.execute_reply":"2024-05-05T19:36:33.639473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:36:33.641622Z","iopub.execute_input":"2024-05-05T19:36:33.642332Z","iopub.status.idle":"2024-05-05T19:36:33.66563Z","shell.execute_reply.started":"2024-05-05T19:36:33.642296Z","shell.execute_reply":"2024-05-05T19:36:33.664569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Running the Cross-Validation Function\n\nWe will find which model yields the highest F1 Score.","metadata":{}},{"cell_type":"code","source":"X = df.drop(columns=[\"Target\"])\ny = df[\"Target\"]\ncross_val_5fold(X, y)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:36:33.667075Z","iopub.execute_input":"2024-05-05T19:36:33.667831Z","iopub.status.idle":"2024-05-05T19:36:33.696461Z","shell.execute_reply.started":"2024-05-05T19:36:33.667785Z","shell.execute_reply":"2024-05-05T19:36:33.695567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Extra Trees Classifier has yielded the best F1 score. We will train the model and send it to competition.","metadata":{}},{"cell_type":"markdown","source":"### Submitting to Competition\nWe will have to create all the features for the test set and then normalize them.","metadata":{}},{"cell_type":"code","source":"df_test['SMA'] = df_test.groupby('Symbol')['Close'].rolling(window=200).mean().reset_index(0, drop=True)\ndf_test.loc[df_test['SMA'].isna(), 'SMA'] = df_test['Close']\ndf_test['EMA'] = df_test.groupby('Symbol')['Close'].apply(lambda x: x.ewm(span=12, adjust=False).mean()).reset_index(0, drop=True)\ndf_test['MACD'] = df_test['EMA'] - df_test.groupby('Symbol')['Close'].apply(lambda x: x.ewm(span=26, adjust=False).mean()).reset_index(0, drop=True)\nchange = df_test.groupby('Symbol')['Close'].diff()\ndf_test['Gain'] = change.apply(lambda x: x if x > 0 else 0)\ndf_test['Loss'] = change.apply(lambda x: -x if x < 0 else 0)\navgGain = df_test.groupby('Symbol')['Gain'].rolling(window=14, min_periods=1).mean().reset_index(drop=True)\navgLoss = df_test.groupby('Symbol')['Loss'].rolling(window=14, min_periods=1).mean().reset_index(drop=True)\ndf_test['RSI'] = 100 - (100 / (1 + avgGain/avgLoss))\ndf_test.loc[df_test['Loss'] < 1e-2, 'RSI'] = 100\ndf_test[\"RSI\"] = df_test.groupby('Symbol')['RSI'].transform(lambda x: x.fillna(x.mean()))\ndf_test = df_test.drop(columns=['Gain', 'Loss'])\ndf_test['Dir'] = change.apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\ndf_test['OBV'] = df_test.groupby('Symbol')['Volume'].transform(lambda x: x * df_test['Dir']).cumsum()\ndf_test = df_test.drop(columns=['Dir'])\nprobs = df_test.groupby('Symbol')['Close'].transform(lambda x: x / x.sum())\ndf_test[\"CE\"] = probs.rolling(window=10).apply(lambda x: -np.sum(x * np.log2(x))).reset_index(drop=True)\ndf_test[\"CE\"] = df_test.groupby('Symbol')['CE'].transform(lambda x: x.fillna(x.mean()))\nopen_mean = df_test.groupby('Symbol')['Open'].rolling(window=20).mean().reset_index(drop=True)\nclose_mean = df_test.groupby('Symbol')['Close'].rolling(window=20).mean().reset_index(drop=True)\ndf_test[\"AOC\"] = open_mean - close_mean\ndf_test[\"AOC\"] = df_test.groupby('Symbol')['AOC'].transform(lambda x: x.fillna(x.mean()))\ndf_test[\"CV\"] = df_test.groupby('Symbol')[\"Close\"].rolling(window=30).var().reset_index(drop=True)\ndf_test[\"CV\"] = df_test.groupby('Symbol')[\"CV\"].transform(lambda x: x.fillna(x.mean()))\ndf_test[\"MV\"] = df_test.groupby('Symbol')[\"Volume\"].rolling(window=15).mean().reset_index(drop=True)\ndf_test[\"MV\"] = df_test.groupby('Symbol')[\"MV\"].transform(lambda x: x.fillna(x.mean()))\ndf_test[\"VV\"] = df_test.groupby('Symbol')[\"Volume\"].rolling(window=30).var().reset_index(drop=True)\ndf_test[\"VV\"] = df_test.groupby('Symbol')[\"VV\"].transform(lambda x: x.fillna(x.mean()))\nhigh_mean = df_test.groupby('Symbol')['High'].rolling(window=20).mean().reset_index(drop=True)\nlow_mean = df_test.groupby('Symbol')['Low'].rolling(window=20).mean().reset_index(drop=True)\ndf_test[\"AHL\"] = high_mean - low_mean\ndf_test[\"AHL\"] = df_test.groupby('Symbol')['AHL'].transform(lambda x: x.fillna(x.mean()))\ndf_test[\"HV\"] = df_test.groupby('Symbol')['High'].rolling(window=20).var().reset_index(drop=True)\ndf_test[\"HV\"] = df_test.groupby('Symbol')['HV'].transform(lambda x: x.fillna(x.mean()))\ndf_test[\"LV\"] = df_test.groupby('Symbol')['Low'].rolling(window=20).var().reset_index(drop=True)\ndf_test[\"LV\"] = df_test.groupby('Symbol')['LV'].transform(lambda x: x.fillna(x.mean()))\nprobs = df_test.groupby('Symbol')['Volume'].transform(lambda x: x / x.sum())\ndf_test[\"VE\"] = probs.rolling(window=10).apply(lambda x: -np.sum(x/np.sum(x) * np.log2(x/np.sum(x)))).reset_index(drop=True)\ndf_test[\"VE\"] = df_test.groupby('Symbol')['VE'].transform(lambda x: x.fillna(x.mean()))\nprobs = df_test.groupby('Symbol')['Open'].transform(lambda x: x / x.sum())\ndf_test[\"OE\"] = probs.rolling(window=10).apply(lambda x: -np.sum(x/np.sum(x) * np.log2(x/np.sum(x)))).reset_index(drop=True)\ndf_test[\"OE\"] = df_test.groupby('Symbol')['OE'].transform(lambda x: x.fillna(x.mean()))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:36:33.69773Z","iopub.execute_input":"2024-05-05T19:36:33.698329Z","iopub.status.idle":"2024-05-05T19:41:09.435152Z","shell.execute_reply.started":"2024-05-05T19:36:33.698302Z","shell.execute_reply":"2024-05-05T19:41:09.434066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = df_test.drop(columns=[\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Date\", \"Volume\"])\n\nfor i in df_test.columns:\n    if i == 'Symbol' or i == 'Id':\n        continue\n    df_test[i] = df_test.groupby('Symbol')[i].transform(lambda x: ((x - x.min() )/ (x.max() - x.min()))).reset_index(drop=True)\n\ndf_test = df_test.drop(columns=['Symbol'])","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:41:09.436482Z","iopub.execute_input":"2024-05-05T19:41:09.436849Z","iopub.status.idle":"2024-05-05T19:41:11.114519Z","shell.execute_reply.started":"2024-05-05T19:41:09.436812Z","shell.execute_reply":"2024-05-05T19:41:11.113563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = df_test.drop(columns=['Id'])\nX_test = X_test.fillna(X_test.mean())\nmodel = ExtraTreesClassifier()\nmodel.fit(X, y)\ny_test = model.predict(X_test)\nsubmission = df_test.loc[:,df_test.columns.isin(('Id', ))]\nsubmission.loc[:,'Target'] = y_test\nsubmission.to_csv(\"submission_2.csv\", index=None)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:41:11.116101Z","iopub.execute_input":"2024-05-05T19:41:11.116489Z","iopub.status.idle":"2024-05-05T19:41:11.121175Z","shell.execute_reply.started":"2024-05-05T19:41:11.116454Z","shell.execute_reply":"2024-05-05T19:41:11.120113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Wrapper Method\nBy training a Linear SVC we will see which features contribute more and which feature contribute less to the F1 score.\nWe will train using 14 features, remove the least contributing one, then 13 features unitl and so on we are left with only 12 features.","metadata":{}},{"cell_type":"code","source":"def wrapper(X, y):\n    model = LinearSVC()\n    print(\"Column\", \"F1 Score\")\n    for column in X.columns:\n        X_wrapper = X.drop(columns=[column])\n        model.fit(X_wrapper, y)\n        y_pred = model.predict(X_wrapper)\n        f1 = f1_score(y, y_pred)\n        print(column, f1)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:41:11.122688Z","iopub.execute_input":"2024-05-05T19:41:11.12304Z","iopub.status.idle":"2024-05-05T19:41:11.134584Z","shell.execute_reply.started":"2024-05-05T19:41:11.123013Z","shell.execute_reply":"2024-05-05T19:41:11.133486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df.drop(columns=[\"Target\"])\ny = df[\"Target\"]\nwrapper(X, y)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:41:11.135907Z","iopub.execute_input":"2024-05-05T19:41:11.136229Z","iopub.status.idle":"2024-05-05T19:41:11.193835Z","shell.execute_reply.started":"2024-05-05T19:41:11.136202Z","shell.execute_reply":"2024-05-05T19:41:11.192755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The feature without which the score is highest is Volume Entropy (VE), we will remove it.","metadata":{}},{"cell_type":"code","source":"X = X.drop(columns=[\"VE\"])\nwrapper(X, y)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:41:11.195365Z","iopub.execute_input":"2024-05-05T19:41:11.196368Z","iopub.status.idle":"2024-05-05T19:41:11.223553Z","shell.execute_reply.started":"2024-05-05T19:41:11.196337Z","shell.execute_reply":"2024-05-05T19:41:11.222519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removing LV, HV, MV and CV yields the same F1 score. We will remove LV.","metadata":{}},{"cell_type":"code","source":"X = X.drop(columns=[\"LV\"])\nwrapper(X, y)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:41:11.224732Z","iopub.execute_input":"2024-05-05T19:41:11.225061Z","iopub.status.idle":"2024-05-05T19:41:11.249937Z","shell.execute_reply.started":"2024-05-05T19:41:11.225034Z","shell.execute_reply":"2024-05-05T19:41:11.249054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MV, HV and CV have the same F1 score, we will now remove MV.","metadata":{}},{"cell_type":"code","source":"X = X.drop(columns=[\"MV\"])\nX","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:41:11.25106Z","iopub.execute_input":"2024-05-05T19:41:11.251369Z","iopub.status.idle":"2024-05-05T19:41:11.288317Z","shell.execute_reply.started":"2024-05-05T19:41:11.251342Z","shell.execute_reply":"2024-05-05T19:41:11.287346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are now left with 12 features.","metadata":{}},{"cell_type":"markdown","source":"### Running the Cross-Validation Function","metadata":{}},{"cell_type":"code","source":"cross_val_5fold(X, y)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:41:11.289508Z","iopub.execute_input":"2024-05-05T19:41:11.289831Z","iopub.status.idle":"2024-05-05T19:41:11.294172Z","shell.execute_reply.started":"2024-05-05T19:41:11.289783Z","shell.execute_reply":"2024-05-05T19:41:11.293045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ExtraTreesClassifier has once again proved to be the best for this.","metadata":{}},{"cell_type":"markdown","source":"### Submitting to Competition","metadata":{}},{"cell_type":"code","source":"X_test = df_test.drop(columns=['VE', 'LV', 'MV', 'Id'])\nX_test = X_test.fillna(X_test.mean())\nmodel = ExtraTreesClassifier()\nmodel.fit(X, y)\ny_test = model.predict(X_test)\nsubmission = df_test.loc[:,df_test.columns.isin(('Id', ))]\nsubmission.loc[:,'Target'] = y_test\nsubmission.to_csv(\"submission.csv\", index=None)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:41:11.295503Z","iopub.execute_input":"2024-05-05T19:41:11.295837Z","iopub.status.idle":"2024-05-05T19:42:33.702331Z","shell.execute_reply.started":"2024-05-05T19:41:11.295789Z","shell.execute_reply":"2024-05-05T19:42:33.701309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Explainability with SHAP","metadata":{}},{"cell_type":"code","source":"explainer = shap.TreeExplainer(model)\nexplainer","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:42:42.781589Z","iopub.execute_input":"2024-05-05T19:42:42.781934Z","iopub.status.idle":"2024-05-05T19:42:48.366332Z","shell.execute_reply.started":"2024-05-05T19:42:42.781905Z","shell.execute_reply":"2024-05-05T19:42:48.365056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will chose 10 rows of which the last 5 is all with Target equal to 1 and the first 5 with at least one with Target equal to 0 ","metadata":{}},{"cell_type":"code","source":"selected = X.iloc[161312:161322]\nprint(df_train.iloc[161312:161322][\"Target\"])\nselected","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:42:48.367483Z","iopub.execute_input":"2024-05-05T19:42:48.367809Z","iopub.status.idle":"2024-05-05T19:42:48.387814Z","shell.execute_reply.started":"2024-05-05T19:42:48.367766Z","shell.execute_reply":"2024-05-05T19:42:48.386703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will extract the shap values and create a force_plot from these 10 samples.","metadata":{}},{"cell_type":"code","source":"shap_values = []\nfor i in range(len(selected)):\n    shap_values += [explainer.shap_values(selected.iloc[i])]\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:42:48.389132Z","iopub.execute_input":"2024-05-05T19:42:48.389436Z","iopub.status.idle":"2024-05-05T19:44:15.458287Z","shell.execute_reply.started":"2024-05-05T19:42:48.38941Z","shell.execute_reply":"2024-05-05T19:44:15.457291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Force Plots","metadata":{}},{"cell_type":"code","source":"shap.initjs()\nfor value in shap_values:\n    display(shap.force_plot(explainer.expected_value[1], value[0]))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:44:15.465647Z","iopub.execute_input":"2024-05-05T19:44:15.466098Z","iopub.status.idle":"2024-05-05T19:44:15.509536Z","shell.execute_reply.started":"2024-05-05T19:44:15.466069Z","shell.execute_reply":"2024-05-05T19:44:15.508518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For samples with Target = 0 the most impactful feature is CV (Close Variance), followed by OBV (On-Balance Volume).\nFor samples with Target = 1 the most impactful feature is EMA (Exponential Moving Average), followed by MACD (Moving average Convergence/Divergence) and AHL (Average High - Low).","metadata":{}},{"cell_type":"code","source":"def visualize(series):\n    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n\n    for col in [\"CV\", \"OBV\", \"MACD\", \"EMA\", \"AHL\"]:\n        ax.plot(range(len(series)), series[col], label=col)\n    ax.set_title(\"10 day visualization\")\n    ax.set_xlabel(\"Day\")\n    ax.set_ylabel(\"Feature\")\n    ax.legend()\n\n    plt.tight_layout()\n    plt.show()\n    \nvisualize(selected)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:44:15.510785Z","iopub.execute_input":"2024-05-05T19:44:15.511134Z","iopub.status.idle":"2024-05-05T19:44:15.914635Z","shell.execute_reply.started":"2024-05-05T19:44:15.511107Z","shell.execute_reply":"2024-05-05T19:44:15.913689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. RIPPER Optimization","metadata":{}},{"cell_type":"markdown","source":"We will choose NVIDIA stock.","metadata":{}},{"cell_type":"code","source":"df_test","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:44:15.916139Z","iopub.execute_input":"2024-05-05T19:44:15.916465Z","iopub.status.idle":"2024-05-05T19:44:15.940302Z","shell.execute_reply.started":"2024-05-05T19:44:15.916438Z","shell.execute_reply":"2024-05-05T19:44:15.939162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df_train.drop(columns=[\"Id\", \"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Date\", \"Volume\", \"VE\", \"LV\", \"MV\"])\nfor i in df.columns:\n    if i == 'Symbol' or i == 'Id':\n        continue\n    df[i] = df_train.groupby('Symbol')[i].transform(lambda x: ((x - x.min() )/ (x.max() - x.min()))).reset_index(drop=True)\n\nX = df[df_train[\"Symbol\"] == \"NVDA\"]\nX","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:44:15.9417Z","iopub.execute_input":"2024-05-05T19:44:15.942169Z","iopub.status.idle":"2024-05-05T19:44:18.523001Z","shell.execute_reply.started":"2024-05-05T19:44:15.942142Z","shell.execute_reply":"2024-05-05T19:44:18.521986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = X[\"Target\"]\nX = X.drop(columns=[\"Target\", \"Symbol\"])\nX","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:44:18.52462Z","iopub.execute_input":"2024-05-05T19:44:18.524925Z","iopub.status.idle":"2024-05-05T19:44:18.546444Z","shell.execute_reply.started":"2024-05-05T19:44:18.524901Z","shell.execute_reply":"2024-05-05T19:44:18.545452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We do a 67%-33% train-test split.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nmodel = lw.RIPPER()\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:44:18.547856Z","iopub.execute_input":"2024-05-05T19:44:18.548367Z","iopub.status.idle":"2024-05-05T19:44:20.829883Z","shell.execute_reply.started":"2024-05-05T19:44:18.548319Z","shell.execute_reply":"2024-05-05T19:44:20.828812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We do a prediction on the test set.","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(X_test)\nf1 = f1_score(y_test, y_pred)\nprint(\"F1 Score:\", f1)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:44:20.831096Z","iopub.execute_input":"2024-05-05T19:44:20.831447Z","iopub.status.idle":"2024-05-05T19:44:20.952728Z","shell.execute_reply.started":"2024-05-05T19:44:20.831417Z","shell.execute_reply":"2024-05-05T19:44:20.95172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also print the rules of the RIPPER model.","metadata":{}},{"cell_type":"code","source":"for rule in model.ruleset_:\n    print(rule)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:44:20.954231Z","iopub.execute_input":"2024-05-05T19:44:20.954645Z","iopub.status.idle":"2024-05-05T19:44:20.960869Z","shell.execute_reply.started":"2024-05-05T19:44:20.954608Z","shell.execute_reply":"2024-05-05T19:44:20.95957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The rules seem to be abundant and convoluted. However it is much more interpretable than those of other models.\n\nFor example let's look at rule [OBV=0.24-0.35^EMA=0.012-0.019], it is applied when OBV is in range $[0.24, 0.35]$ and EMA is in range $[0.012, 0.019]$.","metadata":{}},{"cell_type":"markdown","source":"### Grid search optimization of hyperparameters\nWe will use grid search to find the optimal values of parameters k, prune_size and dl_allowance","metadata":{}},{"cell_type":"code","source":"def grid_search(max_rules=None, max_rule_conds=None):\n    ks = [1, 2, 3, 4, 5]\n    prune_sizes = [0.1, 0.15, 0.2, 0.25, 0.3]\n    dl_allowances = [16, 32, 64, 100, 128, 180, 256]\n\n    best_f1 = 0\n    best_params = None\n\n    print(\"K\", \"prune_size\", \"dl_allowance\", \"F1 Score\")\n    for k in ks:\n        for prune_size in prune_sizes:\n            for dl_allowance in dl_allowances:\n                model = lw.RIPPER(max_rules=max_rules, max_rule_conds=max_rule_conds, k=k, prune_size=prune_size, dl_allowance=dl_allowance)\n                model.fit(X_train, y_train)\n                y_pred = model.predict(X_test)\n                f1 = f1_score(y_test, y_pred)\n\n                print(k, prune_size, dl_allowance, f1)\n\n                if f1 > best_f1:\n                    best_f1 = f1\n                    best_params = {'k': k, 'prune_size': prune_size, 'dl_allowance': dl_allowance}\n    return best_f1, best_params","metadata":{"execution":{"iopub.status.busy":"2024-05-05T20:04:41.406312Z","iopub.execute_input":"2024-05-05T20:04:41.407138Z","iopub.status.idle":"2024-05-05T20:04:41.41499Z","shell.execute_reply.started":"2024-05-05T20:04:41.407104Z","shell.execute_reply":"2024-05-05T20:04:41.413914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_f1, best_params = grid_search()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Best Model Parameters\n","metadata":{}},{"cell_type":"code","source":"print(best_f1)\nbest_params","metadata":{"execution":{"iopub.status.busy":"2024-05-05T20:02:23.449903Z","iopub.status.idle":"2024-05-05T20:02:23.450292Z","shell.execute_reply.started":"2024-05-05T20:02:23.4501Z","shell.execute_reply":"2024-05-05T20:02:23.450117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Best Model Rules","metadata":{}},{"cell_type":"code","source":"model = lw.RIPPER(k=best_params[\"k\"], prune_size=best_params[\"prune_size\"], dl_allowance=best_params[\"dl_allowance\"])\nmodel.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:50:45.42033Z","iopub.execute_input":"2024-05-05T19:50:45.420787Z","iopub.status.idle":"2024-05-05T19:50:46.869251Z","shell.execute_reply.started":"2024-05-05T19:50:45.420752Z","shell.execute_reply":"2024-05-05T19:50:46.868423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for rule in model.ruleset_:\n    print(rule)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T19:50:46.87049Z","iopub.execute_input":"2024-05-05T19:50:46.870893Z","iopub.status.idle":"2024-05-05T19:50:46.87693Z","shell.execute_reply.started":"2024-05-05T19:50:46.870859Z","shell.execute_reply":"2024-05-05T19:50:46.875817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Limited RIPPER\nWe will see what happens when we limit the number of rules to 3 and rule conditions to 2 and see what happens.\n","metadata":{}},{"cell_type":"code","source":"model = lw.RIPPER(max_rules=3, max_rule_conds=2, k=best_params[\"k\"], prune_size=best_params[\"prune_size\"], dl_allowance=best_params[\"dl_allowance\"])\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nf1 = f1_score(y_test, y_pred)\nprint(f1)\nfor rule in model.ruleset_:\n    print(rule)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T20:00:12.806188Z","iopub.execute_input":"2024-05-05T20:00:12.807255Z","iopub.status.idle":"2024-05-05T20:00:13.170319Z","shell.execute_reply.started":"2024-05-05T20:00:12.807221Z","shell.execute_reply":"2024-05-05T20:00:13.169265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The best model's accuracy drops dramatically. We will repeat the grid search.","metadata":{}},{"cell_type":"code","source":"best_f1, best_params = grid_search(max_rules=3, max_rule_conds=2)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T20:04:47.049073Z","iopub.execute_input":"2024-05-05T20:04:47.049777Z","iopub.status.idle":"2024-05-05T20:05:52.407151Z","shell.execute_reply.started":"2024-05-05T20:04:47.049744Z","shell.execute_reply":"2024-05-05T20:05:52.406154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The grid search confirms our suspicion that there is not much that can be done with 3 rules and 2 conditions per rule.","metadata":{}}]}