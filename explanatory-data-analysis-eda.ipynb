{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":73206,"databundleVersionId":8057312,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/velimirkovacic/mn-0036533917-explanatory-data-analysis-eda?scriptVersionId=183358967\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Exploratory data analysis (EDA)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:52.677497Z","iopub.execute_input":"2024-03-30T19:08:52.678105Z","iopub.status.idle":"2024-03-30T19:08:52.685492Z","shell.execute_reply.started":"2024-03-30T19:08:52.678062Z","shell.execute_reply":"2024-03-30T19:08:52.684193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndf_train = pd.read_csv(\"../input/dapprojekt24-1/train.csv\")\ndf_test = pd.read_csv(\"../input/dapprojekt24-1/test.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:52.688326Z","iopub.execute_input":"2024-03-30T19:08:52.688838Z","iopub.status.idle":"2024-03-30T19:08:54.138343Z","shell.execute_reply.started":"2024-03-30T19:08:52.688802Z","shell.execute_reply":"2024-03-30T19:08:54.136519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Dimensionality of the dataset \n","metadata":{}},{"cell_type":"code","source":"#df_train = pd.read_csv('train.csv')  \n#df_test = pd.read_csv(\"test.csv\")\n\nprint(\"Train set\")\nprint(\"Shape:\", df_train.shape)\n\nprint(\"Column names: \", end=\"\")\nfor c in df_train.columns:\n    print(c, end= \", \")\n\nprint(\"\\nUnique stocks:\", df_train[\"Symbol\"].nunique())\n\nprint(\"\\n\")\n\nprint(\"Test set\")\nprint(\"Shape:\", df_test.shape)\n\nprint(\"Column names: \", end=\"\")\nfor c in df_test.columns:\n    print(c, end= \", \")\n\nprint(\"\\nUnique stocks:\", df_test[\"Symbol\"].nunique())\n\n\nif(set(df_test[\"Symbol\"].unique()) == set(df_train[\"Symbol\"].unique())):\n    print(\"\\nUnique stocks are the same among both datasets\")\n\n\ndf_train","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:54.139854Z","iopub.execute_input":"2024-03-30T19:08:54.140276Z","iopub.status.idle":"2024-03-30T19:08:54.876036Z","shell.execute_reply.started":"2024-03-30T19:08:54.140232Z","shell.execute_reply":"2024-03-30T19:08:54.872423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are two datasets, a train set and a test set. The train set consists of 501400 rows (samples) and 10 columns (features). The test set consists of 209600 samples and 9 features (missing the target feature). \n\nThe column (feature) names are: \n* Date\n* Symbol\n* Adj Close\n* Close\n* High\n* Low\n* Open\n* Volume\n* Target\n* Id\n\nThere are 200 different stocks, denoted by their symbol in the Symbol column. They are the same among both datasets.","metadata":{}},{"cell_type":"markdown","source":"## 2. Missing values","metadata":{}},{"cell_type":"code","source":"def find_missing_sections(df):\n    rows_with_missing_values = df.isnull().sum(axis = 1) > 0\n\n    missing_value_indexes = df[rows_with_missing_values].index\n\n    if (len(missing_value_indexes) == 0):\n        return []\n\n    # We find the beginning and and indexes of sections with missing values\n    missing_value_limits = [missing_value_indexes[0]]\n    for i in range(len(missing_value_indexes[1:])):\n        if missing_value_indexes[i] - 1 > missing_value_indexes[i - 1]:\n            missing_value_limits += [missing_value_indexes[i - 1], missing_value_indexes[i]]\n    missing_value_limits += [missing_value_indexes[-1]] \n\n\n    # We transform them into (section beginning, section end) pairs\n    missing_value_sections = []\n    for i in range(0, len(missing_value_limits), 2):\n        missing_value_sections += [(missing_value_limits[i], missing_value_limits[i + 1])]\n\n    return missing_value_sections\n","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:54.877137Z","iopub.status.idle":"2024-03-30T19:08:54.878142Z","shell.execute_reply.started":"2024-03-30T19:08:54.877824Z","shell.execute_reply":"2024-03-30T19:08:54.877847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train\")\n\nmissing_value_sections_train = find_missing_sections(df_train)\nprint(\"Sections with missing values: \", len(missing_value_sections_train))\nfor section in missing_value_sections_train:\n    print(section, \"length:\", section[1] - section[0])\n\n\nprint(\"Test\")\n\nmissing_value_sections_test = find_missing_sections(df_test)\nprint(\"Sections with missing values: \", len(missing_value_sections_test))\nfor section in missing_value_sections_test:\n    print(section, \"length:\", section[1] - section[0])","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:54.879691Z","iopub.status.idle":"2024-03-30T19:08:54.880171Z","shell.execute_reply.started":"2024-03-30T19:08:54.879922Z","shell.execute_reply":"2024-03-30T19:08:54.879941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 27245 rows with one or more missing values in the train set organized in 28 distinct sections. \n* 500 - 515, Only the Volume value is missing. (Replace)\n* 17549 - 18524, a section of 975 days with no values except the date and stock symbol (Erase)\n* 50140 - 51252, a section of 1112 days with no values except the date and stock symbol (Erase)\n* and so on...\n\nFor each section we can either replace the missing values or erase the rows. For the section with 15 missing Volume values it would make sense to just replace the value as it is a rather short section and this will not distort the data too much. For the very long sections which are missing every value we have no other choice than to erase them as replacing them would not give us any useful information in the future analysis of the data.\n\nWe will erase all rows which have no values except the date and stock symbol and search for missing values again. We will not modify the test dataset as we will still have to make predictions even for the samples with missing values.\n","metadata":{}},{"cell_type":"code","source":"print(\"Train\")\ndf_train = df_train.dropna(thresh=8)\nmissing_value_sections_train = find_missing_sections(df_train)\nprint(\"Sections with missing values: \", len(missing_value_sections_train))\nfor section in missing_value_sections_train:\n    print(section, \"length:\", section[1] - section[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:54.881365Z","iopub.status.idle":"2024-03-30T19:08:54.881761Z","shell.execute_reply.started":"2024-03-30T19:08:54.88157Z","shell.execute_reply":"2024-03-30T19:08:54.881587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are still left with the section of 16 rows missing the value for Volume. We will solve this by using linear interpolation to bridge the gap between row 499 and row 516.","metadata":{}},{"cell_type":"code","source":"missing_begin, missing_end = missing_value_sections_train[0]\ndf_train.loc[:, \"Volume\"] = df_train[\"Volume\"].interpolate(method='linear')\nresult_df = df_train.iloc[range(missing_begin - 1, missing_end + 2)]","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:54.883964Z","iopub.status.idle":"2024-03-30T19:08:54.884445Z","shell.execute_reply.started":"2024-03-30T19:08:54.884225Z","shell.execute_reply":"2024-03-30T19:08:54.884243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train\")\ndf_train = df_train.dropna(thresh=8)\nmissing_value_sections_train = find_missing_sections(df_train)\nprint(\"Sections with missing values: \", len(missing_value_sections_train))\nfor section in missing_value_sections_train:\n    print(section, \"length:\", section[1] - section[0])","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:54.885954Z","iopub.status.idle":"2024-03-30T19:08:54.886404Z","shell.execute_reply.started":"2024-03-30T19:08:54.886192Z","shell.execute_reply":"2024-03-30T19:08:54.886209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no more rows with missing values.","metadata":{}},{"cell_type":"markdown","source":"## 3. Duplicates","metadata":{}},{"cell_type":"code","source":"print(\"Train\")\nduplicate_rows = df_train.duplicated()\nprint(\"Numer of duplicate rows:\", len(df_train[duplicate_rows]))\nduplicate_date_symbol = df_train.duplicated(subset=[\"Date\", \"Symbol\"])\nprint(\"Numer of duplicate Date-Symbol pairs:\", len(df_train[duplicate_date_symbol]))\nduplicate_id = df_train.duplicated(subset=[\"Id\"])\nprint(\"Numer of duplicate Id-s:\", len(df_train[duplicate_id]))\n\nprint(\"Test\")\nduplicate_rows = df_test.duplicated()\nprint(\"Numer of duplicate rows:\", len(df_test[duplicate_rows]))\nduplicate_date_symbol = df_test.duplicated(subset=[\"Date\", \"Symbol\"])\nprint(\"Numer of duplicate Date-Symbol pairs:\", len(df_test[duplicate_date_symbol]))\nduplicate_id = df_test.duplicated(subset=[\"Id\"])\nprint(\"Numer of duplicate Id-s:\", len(df_test[duplicate_id]))","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:54.888057Z","iopub.status.idle":"2024-03-30T19:08:54.888455Z","shell.execute_reply.started":"2024-03-30T19:08:54.88826Z","shell.execute_reply":"2024-03-30T19:08:54.888276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no duplicate data.","metadata":{}},{"cell_type":"markdown","source":"## 4. Data validation\n\nThe following must be true for all samples:\n1. Adjusted closing price must be less or equal to the closing price.\n2. Low price must be less or equal to the high price.\n3. Opening and closing price must be in the interval between the low price and the high price.\n4. Volume is an integer.\n5. All values are positive.\n\nIn terms of equations:\n\n* Adj Close &le; Close\n* Low &le; High\n* Open ∈ [Low, High]\n* Close ∈ [Low, High]\n* Volume ∈ $\\mathbb{Z}$","metadata":{}},{"cell_type":"code","source":"def validate_data(df):\n    print(\"Train\")\n    adjc_bigger_c = df[\"Adj Close\"] > df[\"Close\"]\n    print(\"Rows with Adj Close > Close:\", len(df[adjc_bigger_c]))\n    low_high = df[\"Low\"] > df[\"High\"]\n    print(\"Rows with Low > High:\", len(df[low_high]))\n    open_in = (df[\"Open\"] < df[\"Low\"]) | (df[\"Open\"] > df[\"High\"])\n    print(\"Rows with Open < Low or Open > High:\", len(df[open_in]))\n    close_in = ( df[\"Close\"] < df[\"Low\"]) | (df[\"Close\"] > df[\"High\"])\n    print(\"Rows with Close < Low or Close > High:\", len(df[close_in]))\n    negative = (df[\"Adj Close\"] < 0) | (df[\"Close\"] < 0) | (df[\"Open\"] < 0) | (df[\"Low\"] < 0) | (df[\"High\"] < 0)\n    print(\"Rows with negative features:\", len(df[negative]))\n\n\n    print(\"Are they the same? \", end=\"\")\n    print((df[negative].index == df[open_in].index).all() and (df[low_high].index == df[open_in].index).all() and (df[close_in].index == df[open_in].index).all())\n\n    isnt_Z = df[\"Volume\"].apply(lambda x: not x.is_integer())\n    print(\"Rows where Volume isn\\'t an integer:\", len(df[isnt_Z]))\n\n    return negative, isnt_Z\n\n\nissue1, issue2 = validate_data(df_train)\ndf_train[issue1]\n","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:54.889819Z","iopub.status.idle":"2024-03-30T19:08:54.890249Z","shell.execute_reply.started":"2024-03-30T19:08:54.890032Z","shell.execute_reply":"2024-03-30T19:08:54.890051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[issue2]","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:54.891624Z","iopub.status.idle":"2024-03-30T19:08:54.892066Z","shell.execute_reply.started":"2024-03-30T19:08:54.891829Z","shell.execute_reply":"2024-03-30T19:08:54.891846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 110 samples where the High price is negative and about three times higher than the other associated price values. We suspect that the High price is not random but somehow modified from the original price. We can try dividing it by -3 and seeing if the aforementioned constraints are satisfied.\n\nThere are also the 16 samples on which we used linear interpolation to fill the gap, we will simply round these to satify the integer constraint.","metadata":{}},{"cell_type":"code","source":"df_train.loc[issue1, \"High\"] /= (-3)\ndf_train.loc[issue2, \"Volume\"] = df_train.loc[issue2, \"Volume\"].round()\n\n_, _ = validate_data(df_train)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:54.893936Z","iopub.status.idle":"2024-03-30T19:08:54.894631Z","shell.execute_reply.started":"2024-03-30T19:08:54.894425Z","shell.execute_reply":"2024-03-30T19:08:54.894443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dividing the High price by -3 worked very well, we also tried using a smaller value to divide by such as -3.1, this yielded unfavorable results as not all rows satisfied the constraints. This proces that -3 is the smallest negative value we can use in order to \"fix\" the prices.","metadata":{}},{"cell_type":"code","source":"_, _ = validate_data(df_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:54.895902Z","iopub.status.idle":"2024-03-30T19:08:54.896321Z","shell.execute_reply.started":"2024-03-30T19:08:54.896126Z","shell.execute_reply":"2024-03-30T19:08:54.896143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are no problematic rows in the test set.","metadata":{}},{"cell_type":"markdown","source":"## 5. Feature distributions","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(3, 2, figsize=(10, 10))\n\naxes = axes.flatten()\n\ncols = ['Adj Close', 'Close', 'Open', 'High', 'Low', 'Volume']\n\nfor i, col in enumerate(cols):\n    axes[i].hist(df_train[col], bins=100)\n    axes[i].set_title(col + \" histogram\")\n    axes[i].set_xlabel(\"Stock price\")\n    axes[i].set_ylabel(\"Frequency\")\n\nplt.tight_layout()\nplt.show()\n\noutliers = df_train[\"Adj Close\"] > 1000\ndf_train[outliers]","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:54.897441Z","iopub.status.idle":"2024-03-30T19:08:54.898203Z","shell.execute_reply.started":"2024-03-30T19:08:54.89796Z","shell.execute_reply":"2024-03-30T19:08:54.897979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The stock NVR is an outlier as it has a much higher prices than the other stocks. This may not be a problem as it could be solved by good feature extraction. We will visualize the features with a logairthmic scale on the y-axis.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(3, 2, figsize=(10, 10))\n\naxes = axes.flatten()\n\ncols = ['Adj Close', 'Close', 'Open', 'High', 'Low', 'Volume']\n\nfor i, col in enumerate(cols):\n    axes[i].hist(df_train[col], bins=100)\n    axes[i].set_title(col + \" histogram\")\n    axes[i].set_yscale(\"log\")\n    axes[i].set_xlabel(\"Stock price\")\n    axes[i].set_ylabel(\"Frequency (logarithmic)\")\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:54.899902Z","iopub.status.idle":"2024-03-30T19:08:54.900302Z","shell.execute_reply.started":"2024-03-30T19:08:54.900112Z","shell.execute_reply":"2024-03-30T19:08:54.900128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Visualization","metadata":{}},{"cell_type":"markdown","source":"We will visualize price momevemnts of stocks MMM, NVR and GL for 50 days. We will show the opening and closing price as well as the high and low prices.","metadata":{}},{"cell_type":"code","source":"def visualize_stock(symbol, length):\n    stock = df_train[\"Symbol\"] == symbol\n\n    fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n\n    for col in [\"Open\", \"Close\", \"Low\", \"High\"]:\n        ax.plot(df_train[stock][\"Date\"][:length], df_train[stock][col][:length], label=col)\n    ax.set_title(\"50 day plot of stock \" + symbol)\n    ax.set_xlabel(\"Date\")\n    ax.set_xticklabels(df_train[stock][\"Date\"][:length], rotation=45)\n    ax.set_ylabel(\"Price\")\n    ax.legend()\n\n    plt.tight_layout()\n    plt.show()\n\nvisualize_stock(\"MMM\", 50)\nvisualize_stock(\"NVR\", 50)\nvisualize_stock(\"GL\", 50)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:54.901424Z","iopub.status.idle":"2024-03-30T19:08:54.901804Z","shell.execute_reply.started":"2024-03-30T19:08:54.901614Z","shell.execute_reply":"2024-03-30T19:08:54.90163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Best stock to buy on 15.4.2017.","metadata":{}},{"cell_type":"markdown","source":"The question is what makes a stock better to buy than another one. We defined that a stock is worth buying if its price increases at least 2% in the following 2 months. We can thus say that a stock is better to buy if it has the highest price increase in the following 2 months.\n\nThe average daily price is calculated as (High - Low)/2.\n\nThere is no data for days 14.4.2017., 15.4.2017. and 16.4.2017. We will use the average price of 13.4.2017. and 17.4.2017. as the price on day 15.4.2017. Thankfully, the price for the day 15.5.2017. is available.","metadata":{}},{"cell_type":"code","source":"first_date = (df_train[\"Date\"] >= \"2017-04-13\") & (df_train[\"Date\"] <= \"2017-04-17\")\navg_first = df_train[first_date].groupby(\"Symbol\")[[\"High\", \"Low\"]].mean()\navg_first[\"Price\"] = (avg_first[\"High\"] + avg_first[\"Low\"])/2\n\navg_second = df_train[df_train[\"Date\"] == \"2017-06-15\"].groupby(\"Symbol\")[[\"High\", \"Low\"]].mean()\navg_second[\"Price\"] = (avg_second[\"High\"] + avg_second[\"Low\"])/2\n\nprice_change = ((avg_second - avg_first) / avg_first)[[\"Price\"]]\nprice_change.rename(columns={\"Price\": \"Change\"}, inplace=True)\n\n\nstock = price_change[\"Change\"].idxmax()\nincrease = price_change[\"Change\"].max()\nprint(\"Stock:\", stock)\nprint(f\"Increase: {increase:.4f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:54.905452Z","iopub.status.idle":"2024-03-30T19:08:54.905841Z","shell.execute_reply.started":"2024-03-30T19:08:54.905651Z","shell.execute_reply":"2024-03-30T19:08:54.905667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the stock with the symbol NVDA, denoting NVIDIA Corp has had the highest increase in price in the following 2 months, an increae of 0.5479%.","metadata":{}},{"cell_type":"markdown","source":"## 8. Baseline model\n\nWe will first create feature sets that out models will be able to use.\n\nString features (Date and Symbol) will be removed. Id will be removed as it carries no useful information. Adjusted Close is correlated to Close so it will also be removed.\n\nFeatures will be normalized using min-max scaling.","metadata":{}},{"cell_type":"code","source":"X_train = df_train.drop([\"Id\", \"Date\", \"Symbol\", \"Target\", \"Adj Close\"], axis=1)\ny_train = df_train[\"Target\"]\n\nX_test = df_test.drop([\"Id\", \"Date\", \"Symbol\", \"Adj Close\"], axis=1)\n\n#X['Date'], _ = pd.factorize(X['Date'])\n#X['Symbol'], _ = pd.factorize(X['Symbol'])\n\nscaler = MinMaxScaler()\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\nX_test = pd.DataFrame(scaler.fit_transform(X_test), columns=X_test.columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:54.906646Z","iopub.status.idle":"2024-03-30T19:08:54.907054Z","shell.execute_reply.started":"2024-03-30T19:08:54.906839Z","shell.execute_reply":"2024-03-30T19:08:54.906855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use this function to evaluate the models.","metadata":{}},{"cell_type":"code","source":"def classification_metrics(model):\n    y_pred = model.predict(X_train)\n    acc = accuracy_score(y_train, y_pred)\n    print(\"Accuracy:\", acc)\n    cr = classification_report(y_train, y_pred, zero_division=0)\n    print(\"Classification report:\\n\", cr)\n    cm = confusion_matrix(y_train, y_pred)\n    print(\"Confusion matrix:\\n\", cm)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:54.909391Z","iopub.status.idle":"2024-03-30T19:08:54.910051Z","shell.execute_reply.started":"2024-03-30T19:08:54.909736Z","shell.execute_reply":"2024-03-30T19:08:54.90976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8.1. Logistic Regression\n","metadata":{}},{"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(X_train, y_train)\n\nclassification_metrics(model)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:54.911948Z","iopub.status.idle":"2024-03-30T19:08:54.912491Z","shell.execute_reply.started":"2024-03-30T19:08:54.912218Z","shell.execute_reply":"2024-03-30T19:08:54.912239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8.2. XGBoost","metadata":{}},{"cell_type":"code","source":"model = GradientBoostingClassifier() \nmodel.fit(X_train, y_train)\n\nclassification_metrics(model)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:54.913953Z","iopub.status.idle":"2024-03-30T19:08:54.914494Z","shell.execute_reply.started":"2024-03-30T19:08:54.914215Z","shell.execute_reply":"2024-03-30T19:08:54.914236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 8.3. Random Forest","metadata":{}},{"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=128)\nmodel.fit(X_train, y_train)\n\nclassification_metrics(model)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:54.916348Z","iopub.status.idle":"2024-03-30T19:08:54.916906Z","shell.execute_reply.started":"2024-03-30T19:08:54.916609Z","shell.execute_reply":"2024-03-30T19:08:54.91663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, both Logistic Regression and XGBoost classify (almost) every sample into Target = 1 and have a surprisingly high precission because of a disproportion in the number of samples which Target = 0.\n\nRandom Forest, on the other hand, has excellent performance on the train set (likely overfitted) and we will use it to predict the test set.","metadata":{}},{"cell_type":"markdown","source":"#### Prediction\n\nWe have to fill the missing values in the test set in order for Random Forest to properly work. We will use mean fill.","metadata":{}},{"cell_type":"code","source":"X_test = X_test.fillna(X_test.mean())","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:54.918697Z","iopub.status.idle":"2024-03-30T19:08:54.919288Z","shell.execute_reply.started":"2024-03-30T19:08:54.918963Z","shell.execute_reply":"2024-03-30T19:08:54.918985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = model.predict(X_test)\nprint(y_test)","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:54.921823Z","iopub.status.idle":"2024-03-30T19:08:54.92241Z","shell.execute_reply.started":"2024-03-30T19:08:54.922122Z","shell.execute_reply":"2024-03-30T19:08:54.922145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Submission file","metadata":{}},{"cell_type":"code","source":"# create submission format\nsubmission = df_test.loc[:,df_test.columns.isin(('Id', ))]\n\n# add random predictions as 'Predicted' column into submission df\nsubmission.loc[:,'Target'] = y_test\n\n# Save predictions to working directiory - this creates submission file\nsubmission.to_csv(\"submission.csv\", index=None)\n\nsubmission","metadata":{"execution":{"iopub.status.busy":"2024-03-30T19:08:55.09491Z","iopub.status.idle":"2024-03-30T19:08:55.095434Z","shell.execute_reply.started":"2024-03-30T19:08:55.095204Z","shell.execute_reply":"2024-03-30T19:08:55.095223Z"},"trusted":true},"execution_count":null,"outputs":[]}]}